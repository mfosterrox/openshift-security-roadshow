= Lab setup and introduction

== Module goals

* Access all of the applications in the lab environment
* Deploy our default insecure applications
* Build and deploy a "Developer" image to be used in the workshop

== Accessing the workshop components

Let's start by ensuring you have access to all the necessary resources to complete this lab.

TIP: Always open the URL by opening in a new tab. For example on a Mac *<cmd> click*

=== Access the Red Hat^(R)^ OpenShift Container Platform (OCP) user interface

First, make sure you can access the Red Hat^(R)^ OCP console user interface.

*Procedure*

[start=1]
. Log into the OCP console at `{web_console_url}[{web_console_url}^]`
. Click the *rhsso* option

IMPORTANT: If the variables on the screen are unavailable, please inform your workshop administrator.

image::01-ocp-login-admin.png[OpenShift console,link=self, window=blank, width=100%]

[start=3]
. Enter the OCP credentials

[cols="1,1"]
|===
|*User:*| {openshift_admin_user}
|*Password:*| {openshift_admin_password}
|===


[start=4]
. Enter the OpenShift username *{openshift_admin_user}* and password: *{openshift_admin_password}*

image::01-ocp-login-password.png[OpenShift console login,link=self, window=blank, width=100%]

=== Access the Red Hat^(R)^ Advanced Cluster Security (RHACS) user interface

Ensure that you have access to the RHACS user interface.

*Procedure*

[start=1]
. Log into the RHACS console at `{acs_route}[{acs_route}^]`

image::01-rhacs-login.png[RHACS console,link=self, window=blank, width=100%]

[start=2]
. Enter the RHACS credentials

[cols="1,1"]
|===
| *RHACS Console Username:* | {acs_portal_username}
| *RHACS Console Password:* | {acs_portal_password}
|===


image::01-rhacs-console-dashboard.png[RHACS console,link=self, window=blank, width=100%]

=== Access the Red Hat^(R)^ Quay user interface

Red Hat Quay is an enterprise-grade container registry that provides a secure, scalable, and highly available solution for storing and managing container images. It offers integrated security scanning, access controls, and image signing to help ensure the integrity and security of containerized applications across hybrid and multi-cloud environments, including Red Hat OpenShift, AWS, Azure, and Google Cloud.

---

Let's ensure that you have access to the RHACS User Interface.

*Procedure*

[start=1]
. Log into the Quay console at {quay_console_url}[{quay_console_url}^]

image::00-quay-login.png[Quay Console,link=self, window=blank, width=100%]

[start=2]
. Enter the Quay credentials

[source,sh,subs="attributes",role=execute]

[cols="1,1"]
|===
| *Quay Console Username:* | {quay_admin_username}
| *Quay Console Password:* | {quay_admin_password}
|===

=== OpenShift admin access verification

OpenShift admin access verification involves ensuring that users have the appropriate permissions and roles assigned to them for managing the OpenShift cluster. This can be done by checking the user roles and bindings within the cluster. You'll be verifying your permissions using the oc command-line tool.

*Verify access to the OpenShift cluster*

Next, let's switch to the OpenShift cluster running and do our work (for now) in the OpenShift cluster

*Procedure*

[start=1]
. Run the following command

[source,sh,subs="attributes",role=execute]
----
oc config use-context admin
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[lab-user@bastion ~]$oc config use-context eks-admin
Switched to context "admin".
----

[start=2]
. Verify access to the OpenShift cluster and the available nodes

[source,sh,subs="attributes",role=execute]
----
oc whoami
oc get nodes -A
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[lab-user@bastion ~]$ oc whoami
oc get nodes -A
system:admin
NAME                                        STATUS   ROLES                  AGE    VERSION
<OCP_IP>0.us-east-2.compute.internal   Ready    worker                 4h1m   v1.27.11+749fe1d
<OCP_IP>.us-east-2.compute.internal    Ready    control-plane,master   4h7m   v1.27.11+749fe1d
<OCP_IP>.us-east-2.compute.internal    Ready    control-plane,master   4h7m   v1.27.11+749fe1d
<OCP_IP>.us-east-2.compute.internal    Ready    worker                 4h2m   v1.27.11+749fe1d
<OCP_IP>.us-east-2.compute.internal    Ready    control-plane,master   4h7m   v1.27.11+749fe1d
<OCP_IP>.us-east-2.compute.internal    Ready    worker                 4h2m   v1.27.11+749fe1d
----

You will now see the OCP role using the *oc* command, as we are currently working in the OpenShift cluster

=== Verify the roxctl CLI and access to RHACS Central Services

Next, verify that we have access to the RHACS Central Service.

*Procedure*

[start=1]
. Run the following command.

[NOTE]
====
This command uses variables saved in the ~/.bashrc file to authenticate with the RHACS Central Service.
====

[source,sh,subs="attributes",role=execute]
----
roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" central whoami
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
UserID:
	auth-token:718744a9-9548-488b-a8b9-07b2c59ea5e6
User name:
	anonymous bearer token "pipelines-ci-token" with roles [Admin] (jti: 718744a9-9548-488b-a8b9-07b2c59ea5e6, expires: 2025-04-03T15:15:06Z)
Roles:
	- Admin
Access:
	rw Access
	rw Administration
	rw Alert
	rw CVE
	rw Cluster
	rw Compliance
	rw Deployment
	rw DeploymentExtension
	rw Detection
	rw Image
	rw Integration
	rw K8sRole
	rw K8sRoleBinding
	rw K8sSubject
	rw Namespace
	rw NetworkGraph
	rw NetworkPolicy
	rw Node
	rw Secret
	rw ServiceAccount
	rw VulnerabilityManagementApprovals
	rw VulnerabilityManagementRequests
	rw WatchedImage
	rw WorkflowAdministration
----

IMPORTANT: This output is showing that you have unrestricted access to the RHACS product. These permissions can be seen in the **RHACS Access Control** tab that we will review later.

image::00-acs-access-control.png[RHACS access control,link=self, window=blank, width=100%]

*Great job!*

== Deploy the vulnerable workshop applications

You now have access to the core OpenShift applications. Next, you'll deploy several insecure apps into the OpenShift cluster. Our insecure demo applications come from a variety of public GitHub repositories and sources. These apps will serve as the main use cases you examine during the workshop, ranging from well-known Capture the Flag (CTF) applications to vulnerabilities like Log4Shell and Apache Struts.

After deploying the applications, you'll scan some of these containers using the roxctl CLI to get a feel for what you'll be dealing with when you get to the security section with Red Hat Advanced Cluster Security

=== Time to deploy

*Procedure*

[start=1]
. Run the following commands in the terminal, one after the other.

====
This command downloads a repository container dockerfiles, attack scripts, and Kubernetes manifests that you will use to deploy the containerized applications to OpenShift.
====

[source,sh,subs="attributes",role=execute]
----
git clone https://github.com/mfosterrox/demo-apps.git demo-apps
----

====
This command sets the variable TUTORIAL_HOME to equal the working directory, allowing you to make references to various files easily.
====

[source,sh,subs="attributes",role=execute]
----
echo export TUTORIAL_HOME="$(pwd)/demo-apps" >> ~/.bashrc
export TUTORIAL_HOME="$(pwd)/demo-apps"
----

====
This command applies the manifests into the OpenShift environment.
====

[source,sh,subs="attributes",role=execute]
----
oc apply -f $TUTORIAL_HOME/kubernetes-manifests/ --recursive
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ oc apply -f $TUTORIAL_HOME/kubernetes-manifests/ --recursive
namespace/operations created
namespace/backend created
....
route.route.openshift.io/webgoat created
configmap/webgoat-config created
----

NOTE: You should see warnings such as: *Warning: would violate PodSecurity "baseline:latest": privileged (container "proxy" must not set securityContext.privileged=true)*. This is because you are deploying flawed container configurations and vulnerable container applications into the OpenShift cluster.

====
The following command will watch all of the applications as they are successfully deployed into the Openshift cluster. You can press ctrl+c to terminate the command.
====

[source,bash,role="execute"]
----
oc get deployments -l demo=roadshow -A -w
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ oc get deployments -l demo=roadshow -A
NAMESPACE    NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
backend      api-server            1/1     1            1           18s
default      dvma                  1/1     1            1           76s
default      juice-shop            1/1     1            1           73s
default      log4shell             1/1     1            1           70s
default      open-api-server       1/1     1            1           39s
default      reporting             1/1     1            1           42s
default      vulnerable-node-app   1/1     1            1           36s
default      webgoat               1/1     1            1           33s
frontend     asset-cache           1/1     1            1           66s
medical      reporting             1/1     1            1           58s
operations   jump-host             1/1     1            1           54s
payments     visa-processor        1/1     1            1           52s
----

IMPORTANT: Please ensure the deploy application are deployed and available before moving onto the next module.

[start=2]
. Run the following roxctl commands to check on the vulnerability status of the applications you deployed.

====
The following command triggers a vulnerability scan by RHACS, roxctl filters the results into a table. The severity flag means only the critical vulnerabilities will be shown. This image is known as the "Damn Vulnerable Wed Application" and it contains A LOT of vulnerabilities.
====

[source,sh,subs="attributes",role=execute]
----
roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" image scan --image=quay.io/mfoster/dvwa --force -o table
----

TIP: The following output can be configured using flags. You can configure different outputs (table, CSV, JSON, and sarif.) and filter for specific severities.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" image scan --image=quay.io/mfoster/dvwa --force -o table
Scan results for image: quay.io/mfoster/dvwa

---------------------------------+------------------+
|          zlib1g           |     1:1.2.13.dfsg-1     |   CVE-2023-45853    | CRITICAL  |         https://nvd.nist.gov/vuln/detail/CVE-2023-45853         |       -       |
+---------------------------+                         +---------------------+-----------+-----------------------------------------------------------------+---------------+
|        zlib1g-dev         |                         |   CVE-2023-45853    | CRITICAL  |         https://nvd.nist.gov/vuln/detail/CVE-2023-45853         |       -       |
+---------------------------+-------------------------+---------------------+-----------+-----------------------------------------------------------------+---------------+
WARN:   A total of 1265 unique vulnerabilities were found in 106 components
----

image::https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExbnY0NDA0ZnJqNXh6cGNqeHNxZGd5Zm5qMnlpOHhrbm1hY2pwcG5ydSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/p18ohAgD3H60LSoI1C/giphy.gif[link=self, window=blank, width=100%, class="center"]

[[build-a-container-image]]

== Build the main workshop application

=== Login to Quay from the CLI

*Procedure*

[start=1]
. Run the following command.

====
Let's export a few variables to make things easier. These variables will stay in the .bashrc file so they're saved in case you need to refresh the terminal.
====

[source,sh,subs="attributes",role=execute]
----
echo export QUAY_USER={quay_admin_username} >> ~/.bashrc
QUAY_USER={quay_admin_username}
----

[start=2]

. Run the following command to set the Quay URL variable

[source,sh,subs="attributes",role=execute]
----
echo export QUAY_URL=$(oc -n quay-enterprise get route quay-quay -o jsonpath='{.spec.host}') >> ~/.bashrc
QUAY_URL=$(oc -n quay-enterprise get route quay-quay -o jsonpath='{.spec.host}')
----

IMPORTANT: Verify that the variables are correct

[source,sh,subs="attributes",role=execute]
----
echo $QUAY_USER
echo $QUAY_URL
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[lab-user@bastion ~]$ echo $QUAY_USER
echo $QUAY_URL
quayadmin
quay-bq65l.apps.cluster-bq65l.bq65l.sandbox209.opentlc.com
----

[start=3]
. Using the terminal on the bastion host, login to quay using the Podman CLI as shown below:

[source,sh,subs="attributes",role=execute]
----
podman login $QUAY_URL -u $QUAY_USER -p {quay_admin_password}
----

NOTE: Use the quay admin credentials to sign in if you run into issues.

[cols="2,2"]
|===
| *Quay Username:* | *{quay_admin_username}*
| *Quay Password:* | *{quay_admin_password}*
|===

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
Username: quayadmin
Password:
Login Succeeded!
----

Beautiful! We're logged in. Now you can download build and push images directly to your quay repository.

[[golden-image]]

=== Create our developer golden image

Imagine a base image as a blank canvas, providing a reliable foundation like a minimal operating system or runtime environment. Developers add the essential configurations, libraries, and tools needed to fit their application's unique demands. By tagging this customized creation as a golden image, developers ensure every deployment uses the same standardized, consistent setup, simplifying workflows and avoiding unexpected surprises.

*Procedure*

. Run the following commands in succession

[source,sh,subs="attributes",role=execute]
----
podman pull python:3.12-alpine
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ podman pull python:3.12-alpine
Trying to pull docker.io/library/python:3.12-alpine...
Getting image source signatures
Copying blob e846654b130a skipped: already exists
Copying blob 00bb69fc5235 skipped: already exists
Copying blob 1f3e46996e29 skipped: already exists
Copying blob 7b104e645578 skipped: already exists
Copying config 451fdb03e1 done   |
Writing manifest to image destination
451fdb03e17285d3263d88f0f3bed692c82556935490d233096974386cd0381b
----

====
This command tags the python:3.12-alpine image with your Quay repository and username allowing you to accurately push the image in a later step.
====

[source,sh,subs="attributes",role=execute]
----
podman tag docker.io/library/python:3.12-alpine $QUAY_URL/$QUAY_USER/python-alpine-golden:0.1
----

====
This command shows you the local images available
====

[source,sh,subs="attributes",role=execute]
----
podman images
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ podman images
REPOSITORY                                                                                  TAG          IMAGE ID      CREATED      SIZE
quay-w2t4c.apps.cluster-w2t4c.w2t4c.sandbox1647.opentlc.com/quayadmin/python-alpine-golden  0.1          451fdb03e172  5 weeks ago  50.6 MB
docker.io/library/python                                                                    3.12-alpine  451fdb03e172  5 weeks ago  50.6 MB
----

====
This command pushes the golden image to your repository
====

[source,sh,subs="attributes",role=execute]
----
podman push $QUAY_URL/$QUAY_USER/python-alpine-golden:0.1
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ podman push $QUAY_URL/$QUAY_USER/python-alpine-golden:0.1
Getting image source signatures
Copying blob c6121fb0c7de done   |
Copying blob a0904247e36a done   |
Copying blob 5b6d0b88dd08 done   |
Copying blob 375bee7d34e2 done   |
Copying config 451fdb03e1 done   |
Writing manifest to image destination
----

Perfect!

[[dev-app]]

=== Create the frontend application, and upload it to Quay

In this section, you will add the application code to the image, build, tag and push the new image to Quay. Later, we'll use that image to deploy an application to the OpenShift Cluster.

TIP: With the variables saved in the ~/.bashrc file you will not have to declare them again in the future.

*Procedure*

. List the contents of the `frontend` directory inside the `app-images` folder to see the files available for the project.

[source,sh,subs="attributes",role=execute]
----
ls $TUTORIAL_HOME/app-images/frontend/
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ ls $TUTORIAL_HOME/app-images/frontend/
Dockerfile  main.py  static
----

[start=2]
. Display the contents of the `Dockerfile` to understand how the image is built, including installed dependencies and the copied files.

[source,sh,subs="attributes",role=execute]
----
cat $TUTORIAL_HOME/app-images/frontend/Dockerfile
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ cat $TUTORIAL_HOME/app-images/frontend/Dockerfile
FROM python:3.12-alpine AS build

# Install Bash and other dependencies
RUN apk add --no-cache bash

# Install required Python packages
RUN pip install --no-cache-dir httpx psycopg psycopg_binary psycopg_pool starlette sse_starlette uvicorn

# Install vulnerable packages for testing
RUN pip install --no-cache-dir setuptools==39.1.0
RUN pip install --no-cache-dir Flask==0.5
RUN pip install --no-cache-dir Django==1.11.29
RUN pip install --no-cache-dir requests==2.19.0
RUN pip install --no-cache-dir PyYAML==3.12

FROM python:3.12-alpine AS run

# Install Bash in the runtime container
RUN apk add --no-cache bash

# Create user and set permissions
RUN adduser -S fritz -G root
USER fritz

# Copy files from the build stage
COPY --from=build /usr/local/lib/python3.12/site-packages /usr/local/lib/python3.12/site-packages
COPY --chown=fritz:root static /home/fritz/static
COPY --chown=fritz:root main.py /home/fritz/main.py

# Expose the port and set the working directory
EXPOSE 8080
WORKDIR /home/fritz

# Set the entry point to start the application
ENTRYPOINT ["python", "main.py"]
----


[start=3]
. Update the `FROM` statement in the Dockerfile to reference a custom base image hosted in a private registry, using `sed` to modify the line.

[source,sh,subs="attributes",role=execute]
----
sed -i "s|^FROM python:3\.12-alpine AS \(\w\+\)|FROM $QUAY_URL/$QUAY_USER/python-alpine-golden:0.1 AS \1|" $TUTORIAL_HOME/app-images/frontend/Dockerfile
----

[start=4]
. Check the Dockerfile again to verify that the `FROM` statement has been updated correctly.

[source,sh,subs="attributes",role=execute]
----
cat $TUTORIAL_HOME/app-images/frontend/Dockerfile
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ cat $TUTORIAL_HOME/app-images/frontend/Dockerfile
FROM quay-w2t4c.apps.cluster-w2t4c.w2t4c.sandbox1647.opentlc.com/quayadmin/python-alpine-golden:0.1 AS build

RUN pip install --no-cache-dir

....
----

[start=5]
. Build the Docker image using `podman` from the `frontend` directory. The `-t` flag tags the image with a version (`0.1`) and a registry URL.

[source,sh,subs="attributes",role=execute]
----
cd $TUTORIAL_HOME/app-images/frontend/
podman build -t $QUAY_URL/$QUAY_USER/frontend:0.1 .
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion frontend]$ podman build -t $QUAY_URL/$QUAY_USER/frontend:0.1 .
[1/2] STEP 1/2: FROM quay-w2t4c.apps.cluster-w2t4c.w2t4c.sandbox1647.opentlc.com/quayadmin/python-alpine-golden:0.1 AS build
.
.
.
Successfully tagged quay-w2t4c.apps.cluster-w2t4c.w2t4c.sandbox1647.opentlc.com/quayadmin/frontend:0.1
46ea42cba3f17c366b0c534164ae088719266df9ab4122532b0bffd1bbefaec9
----

[start=6]
. Upload the built image to a remote registry using `podman push`. The `--remove-signatures` flag ensures that image signatures are not included in the pushed image.

[source,sh,subs="attributes",role=execute]
----
podman push $QUAY_URL/$QUAY_USER/frontend:0.1
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion frontend]$ podman push $QUAY_URL/$QUAY_USER/frontend:0.1 --remove-signatures
Copying blob e2adcecab318 done   |
Copying blob 9944062081bf done   |
Copying blob 83e98ac5789e skipped: already exists
Copying blob 32fd82e104c5 skipped: already exists
Copying blob 453d5d1264c7 done   |
Copying blob 3c37dc31320d done   |
Copying blob 210a2ae1a75e skipped: already exists
Copying blob 57a6ec527341 skipped: already exists
Copying config 46ea42cba3 done   |
Writing manifest to image destination
----

NOTE: Quay will automatically create a private registry to store the application image because of your admin access.

The frontend application is finally built! The next step is to review it in Quay and deploy it to OpenShift using ACM and OpenShift GitOps.

[[quay]]

== Red Hat Quay

Red Hat Quay is an enterprise-quality registry for building, securing and serving container images. It provides secure storage, distribution, governance of containers and cloud-native artifacts on any infrastructure.

=== Accessing Quay

Your Red Hat Quay console is available at: {quay_console_url}[window=blank]

Administrator login is available with:

[source,sh,subs="attributes",role=execute]

[cols="1,1"]
|===
| *Quay Console Username:* | {quay_admin_username}
| *Quay Console Password:* | {quay_admin_password}
|===

[[navigating-the-registry]]

=== Browse the registry

In the setup module you downloaded built and pushed a insecure java application called *frontend*. Now it's time to deploy it to the OpenShift Cluster. To do this you will need to make the registry that you created public.

Let's take a look at our application in the registry.

*Procedure*

. First, click on the *frontend* repository.

image::00-quay-login.png[link=self, window=blank, width=100%]

The information tab shows you information such as

- Podman and Docker commands
- Repository activity
- The repository description.

image::00-frontend-repo.png[link=self, window=blank, width=100%]

====
On the left hand side of the window you should see the following icons labelled in order from top to bottom,
====

image::00-quay-sidebar.png[link=self, window=blank, width=100%]

- Information
- Tags
- Tag History
- Usage Logs
- Settings


[start=2]
. Click on the *Tags* icon.

image::00-quay-tags.png[link=self, window=blank, width=100%]

This tab displays all of the images and tags that have been upladed, providing information such as fixable vulnerabilities, the image size and allows for bulk changes to images based on the security posture.

We will explore this tab a little later in this module.

[start=3]
. Click on the *Tags History* icon.

image::00-quay-history.png[link=self, window=blank, width=100%]

This tab simply displays the container images history over time.

[start=4]
. Click into the SHA256 hash number.

image::00-image-details.png[link=self, window=blank, width=100%]

From this dashboard you will be able to see the image manifest of that container image.

image::00-image-manifest.png[link=self, window=blank, width=100%]

[start=5]
. Click the *BACK* icon in the top left of the dashboard then click on the *Usage Logs* icon.

image::00-quay-back.png[link=self, window=blank, width=100%]
image::00-quay-usage.png[link=self, window=blank, width=100%]


This tab displays the usage over time along with details about who/how the images were pushed to the cluster.

====
You should see that you (The "quayadmin") pushed an image tagged 0.1 to the repository today.
====

[start=6]
. Lastly click on the *Settings* icon.

image::00-quay-settings.png[link=self, window=blank, width=100%]

In this tab you can add/remove users and update permissions, alter the privacy of the repository, and even schedule alerts based on found vulnerabilities.

[start=7]
. Make your repository public before deploying our application in the next step by clicking the *Make Public* button under *Repository Visibility*

image::00-quay-public.png[link=self, window=blank, width=100%]

[start=8]
. Click OK

image::00-quay-public-yes.png[link=self, window=blank, width=100%]

[[vulnerability-scanning-with-quay]]

=== Vulnerability Scanning with Quay

Red Hat Quay can also help with securing our environments by performing a container vulnerability scan on any images added to our registry, and advise which ones are potentially fixable. This feature is also known as vulnerability scanning at rest.

Use the following procedure to check the security scan results for our Java container image you have uploaded.

*Procedure*

. Click on the *Tags* icon on the left side of the screen like before.

image::00-quay-tags.png[link=self, window=blank, width=100%]

NOTE: You may need to click the checkbox near the image you would would like more information on, but the column for *Security Scan* should populate.

[start=2]
. By default, the security scan color codes the vulnerabilities, you can hover over the security scan for more information.

image::00-quay-scan-hover.png[link=self, window=blank, width=100%]

NOTE: The alpine container image you are using in this lab shows 34 vulnerabilities, with 2 cirtical vulnerabilities. This number will change with time and will be different between container scanners for a variety of reasons such as reporting mechanisms, vulnerability feeds and operating system support.

[start=3]
. Click on the list of vulnerabilities to see a more detailed view.

image::00-quay-security.png[link=self, window=blank, width=100%]

image::00-quay-vuln-overview.png[link=self, window=blank, width=100%]

[start=4]
. Click on a vulnerable package on the left menu to get more information about the vulnerability and see what you have to do to fix the issue.

image::00-quay-vuln-detailed.png[link=self, window=blank, width=100%]

NOTE: Toggling for fixable/unfixable vulnerabilities is an excellent way for developers to understand what is within their responsibility for fixing. For example, since you are using an older version of Java, many fixes are available for these common issues.

=== Ensure ACS can pull the image manifest from your Quay instance

*Procedure*

Run the following command to test the RHACS/Quay integration

[source,sh,subs="attributes",role=execute]
----
roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" image scan --image=$QUAY_URL/$QUAY_USER/frontend:0.1 --force -o table
----

TIP: The following output can be configured using flags. You can configure different outputs (table, CSV, JSON, and sarif.) and filter for specific severities.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
.
.
.
|            |         | GHSA-r64q-w8jr-g9qp |    LOW    |             https://github.com/urllib3/urllib3/issues/1553              |    1.24.3     |
+------------+---------+---------------------+-----------+-------------------------------------------------------------------------+---------------+
WARN:   A total of 38 unique vulnerabilities were found in 8 components
----

=== Deploy your newly built application

The last step is to deploy the skupper application with your specific frontend container. You will do this buy altering an existing manifest.

*Procedure*

. Run the following commands in the terminal, one after the other.

====
This commands sets new variables for the skupper example and clones the manifests
====

[source,sh,subs="attributes",role=execute]
----
git clone https://github.com/mfosterrox/skupper-security-demo.git skupper-app
echo export APP_HOME="$(pwd)/skupper-app" >> ~/.bashrc
export APP_HOME="$(pwd)/skupper-app"
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
Cloning into 'skupper-app'...
remote: Enumerating objects: 41, done.
remote: Counting objects: 100% (41/41), done.
remote: Compressing objects: 100% (29/29), done.
remote: Total 41 (delta 17), reused 31 (delta 11), pack-reused 0 (from 0)
Receiving objects: 100% (41/41), 10.93 KiB | 10.93 MiB/s, done.
Resolving deltas: 100% (17/17), done.
----

====
Next, swap the default image repo for the local Quay one container the frontend app you built
====

[source,sh,subs="attributes",role=execute]
----
sed -i "s|quay.io/mfoster/frontend:latest|$QUAY_URL/$QUAY_USER/frontend:0.1|g" $APP_HOME/skupper-demo/frontend.yml
----

[.console-output]
[source,yaml,subs="+macros,+attributes"]
----
[lab-user@bastion skupper-demo]$ sed -i "s|quay.io/mfoster/frontend:latest|$QUAY_URL/$QUAY_USER/frontend:0.1|g" $APP_HOME/skupper-demo/frontend.yml
----

====
Verify the swap was successful
====

[source,sh,subs="attributes",role=execute]
----
cat $APP_HOME/skupper-demo/frontend.yml
----

[.console-output]
[source,yaml,subs="+macros,+attributes"]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: frontend
    demo: roadshow
  name: frontend
  namespace: patient-portal
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      containers:
        - name: frontend
          image: quay-h2xfr.apps.cluster-h2xfr.h2xfr.sandbox2437.opentlc.com/quayadmin/frontend:0.1

		  (For example: Yours will be different)

          imagePullPolicy: Always
          env:
----

====
Next, deploy the application with your updated manifests
====

[source,sh,subs="attributes",role=execute]
----
oc apply -f $APP_HOME/skupper-demo/
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
namespace/patient-portal created
deployment.apps/database created
service/database created
deployment.apps/frontend created
route.route.openshift.io/frontend-patient-route created
service/frontend-service created
deployment.apps/payment-processor created
service/payment-service created
----

====
And lastly, verify that the application is running.
====

[source,sh,subs="attributes",role=execute]
----
oc get pods -n patient-portal
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[lab-user@bastion skupper-demo]$ oc get pods -n patient-portal
NAME                                 READY   STATUS    RESTARTS   AGE
database-758f8f75f7-46ndf            1/1     Running   0          56s
frontend-5c487dcb47-7gsm7            1/1     Running   0          54s
frontend-5c487dcb47-f25hm            1/1     Running   0          54s
frontend-5c487dcb47-z9stn            1/1     Running   0          54s
payment-processor-6644dfc5b7-5vzqh   1/1     Running   0          51s
payment-processor-6644dfc5b7-96jb6   1/1     Running   0          51s
payment-processor-6644dfc5b7-9vl7b   1/1     Running   0          51s
----

== Summary

image::https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExbnY0NDA0ZnJqNXh6cGNqeHNxZGd5Zm5qMnlpOHhrbm1hY2pwcG5ydSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/p18ohAgD3H60LSoI1C/giphy.gif[link=self, window=blank, width=100%, class="center"]

*Great Job!*

Now that the setup is all done, you are ready to move on with RHACS and dive into the security considerations.

On to *Visibility and Navigation*!!